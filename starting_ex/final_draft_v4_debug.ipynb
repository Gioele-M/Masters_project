{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sys import stderr, stdout\n",
    "import tempfile\n",
    "import subprocess\n",
    "from warnings import catch_warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO, SeqRecord, Seq, SearchIO, AlignIO, Phylo\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "import Bio.Entrez\n",
    "from Bio.Phylo.TreeConstruction import DistanceCalculator,DistanceTreeConstructor\n",
    "from pandas.core.frame import DataFrame\n",
    "import logging\n",
    "import time\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to open fasta file of imput\n",
    "def open_fasta(filename) -> SeqRecord:\n",
    "    with open(filename) as handle:\n",
    "        sequence_record = SeqIO.read(handle, 'fasta')\n",
    "    logging.info('Opened sequence {}'.format(sequence_record.id))\n",
    "    return sequence_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to run BLAST with taxid list\n",
    "def blastp_with_list(sequence, list_taxid = [], query_size = 200):\n",
    "    result_handler, result_storer = None, None\n",
    "    #If list is empty run query without specific taxid\n",
    "    if len(list_taxid) <1:\n",
    "        result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, hitlist_size=query_size) \n",
    "        result_storer = result_handler.read()\n",
    "    #Prepare string of Entrez and parse it to qblast\n",
    "    else:\n",
    "        entrez_query = ''\n",
    "        for taxid in list_taxid:\n",
    "            entrez_query += 'txid{}[ORGN]'.format(taxid)\n",
    "            if taxid != list_taxid[-1]:\n",
    "                entrez_query += ' OR '\n",
    "        result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, entrez_query= entrez_query, hitlist_size=query_size)\n",
    "        result_storer = result_handler.read()\n",
    "    logging.info('BLASTp specifying {} taxid(s) completed'.format(len(list_taxid)))\n",
    "\n",
    "    return result_storer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Single blast query for threading\n",
    "#Threading to be implemented \n",
    "def blastp_single_taxid(sequence, taxid, query_size = 20):\n",
    "    entrez_query = 'txid{}[ORGN]'.format(taxid)\n",
    "    result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, entrez_query= entrez_query, hitlist_size=query_size)    \n",
    "    result_storer = result_handler.read()\n",
    "    return result_storer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to create a temporary file from string and read it with SearchIO.read\n",
    "def xml_string_to_handler(string):\n",
    "    #make temporary file\n",
    "    tmp = tempfile.NamedTemporaryFile(mode='a+')\n",
    "    #write string\n",
    "    tmp.write(string)\n",
    "    handler = SearchIO.read(tmp.name, 'blast-xml')\n",
    "    tmp.close()\n",
    "    logging.info('Blast returned {} results'.format(len(handler)))\n",
    "    return handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of a dictionary with all HSPS\n",
    "def blast_to_dictionary(blastresult):\n",
    "    blast_dictionary = {'ID' : [], 'Description' : [], 'Seq_length' : [], 'Accession' : [], 'Bitscore' : [], 'Evalue' : [], 'Tot_aln_span':[], 'Identity' :[]}\n",
    "    #Loop through results \n",
    "    for result in blastresult:\n",
    "        blast_dictionary['ID'].append(result.id)\n",
    "        blast_dictionary['Description'].append(result.description)\n",
    "        blast_dictionary['Seq_length'].append(result.seq_len)\n",
    "        blast_dictionary['Accession'].append(result.accession)\n",
    "        #Store results of first HSP\n",
    "        first_hsp = result.hsps[0]\n",
    "        blast_dictionary['Bitscore'].append(first_hsp.bitscore)\n",
    "        blast_dictionary['Evalue'].append(first_hsp.evalue)\n",
    "        #Create variables to store results of multiple hsps\n",
    "        all_alnspan, all_gapnum = [],[] \n",
    "        #Collect data of all hsps for each hit\n",
    "        for hsp in result.hsps:\n",
    "            all_alnspan.append(int(hsp.aln_span))\n",
    "            all_gapnum.append(int(hsp.gap_num))\n",
    "        #Calculate total alignment span and gaps to calculate identity\n",
    "        tot_alnspan, tot_gapnum = int(), int()\n",
    "        seq_len = int(result.seq_len) #DOUBLE CHECK \n",
    "        for span in all_alnspan:\n",
    "            tot_alnspan += span\n",
    "        for gap in all_gapnum:\n",
    "            tot_gapnum += gap\n",
    "        identity = ((tot_alnspan - tot_gapnum)/seq_len)*100\n",
    "        blast_dictionary['Tot_aln_span'].append(tot_alnspan)\n",
    "        blast_dictionary['Identity'].append(round(identity, 3))\n",
    "\n",
    "    logging.info(\"{} entries were recorded from the BLASTp results\".format(len(blast_dictionary['ID'])))\n",
    "    return blast_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter DF based on Evalue, sequence length and identity\n",
    "def filter_df_blast(df:DataFrame, query:SeqRecord, evalue = 10**-10, difference_from_query = 50, identity_threshold=50) -> DataFrame:\n",
    "    #Record initial dataframe length\n",
    "    initial_len = len(df)\n",
    "\n",
    "    remove_index = []\n",
    "    #Check if main HSP is significant for threshold, store indexes of non-significant hits\n",
    "    for i in range(len(df.index)):\n",
    "        eval = df.iloc[i]['Evalue']\n",
    "        if float(eval) > evalue:\n",
    "            remove_index.append(str(i))\n",
    "        \n",
    "    #Copy DF\n",
    "    df_to_return = df    \n",
    "    #Remove indexes if list is not empty\n",
    "    if len(remove_index)>0:\n",
    "        df_to_return = df_to_return.drop(df.index[int(remove_index)])\n",
    "        df_to_return = df_to_return.reset_index(drop=True)\n",
    "    \n",
    "    #Filter by +/- % of query length\n",
    "    #Obtain query length\n",
    "    query_length = len(query.seq)\n",
    "    #Determine upper/lower threshold of acceptance for sequences \n",
    "    lower, upper = query_length*(difference_from_query/100), query_length*((difference_from_query + 100)/100)\n",
    "    #Filter DF\n",
    "    df_to_return = df_to_return[df_to_return['Seq_length'] > lower]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "    df_to_return = df_to_return[df_to_return['Seq_length'] < upper]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "\n",
    "    #Filter by identity\n",
    "    df_to_return = df_to_return[df_to_return['Identity'] > identity_threshold]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "    \n",
    "    #Calculate how many sequences were removed\n",
    "    final_len = len(df_to_return)\n",
    "\n",
    "    logging.info(\"{} sequences were removed filtering the BLAST results DF, returning a DF with {} entries\".format((initial_len-final_len),final_len))\n",
    "    return df_to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve all result's entries from efetch\n",
    "def retrieve_all_efetch(list_of_entries, email):\n",
    "    Bio.Entrez.email = email\n",
    "    list_of_sequences = []\n",
    "    for entry in list_of_entries:\n",
    "        handler = Bio.Entrez.efetch(db='protein', id=entry, rettype = 'fasta',retmode = 'xml', retmax=1) #Returns JSON regardless\n",
    "        gb_info = Bio.Entrez.read(handler, 'text')#Returns nested lists and dictionaries \n",
    "        list_of_sequences.append(gb_info)\n",
    "    logging.info('{} protein entries were retrieved from NCBI protein database'.format(len(list_of_sequences)))\n",
    "    return list_of_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Efetch to dictionary parser\n",
    "def efetch_protein_to_dictionary(list_of_efetch):\n",
    "    #Declare new dictionary\n",
    "    dictionary = {'Accession':[],'Protein_ID':[], 'Taxid':[], 'Organism_name':[], 'Description':[], 'Seq_length':[], 'Prot_sequence':[]}\n",
    "    for wrapper in list_of_efetch:\n",
    "        try:\n",
    "            #Cast into dictionary to avoid random exception\n",
    "            result = dict(wrapper[0])\n",
    "            acc_ver = result['TSeq_accver']\n",
    "            accession = acc_ver.split('.')\n",
    "            dictionary['Accession'].append(accession[0])\n",
    "            dictionary['Protein_ID'].append(result['TSeq_accver'])\n",
    "            dictionary['Taxid'].append(result['TSeq_taxid'])\n",
    "            dictionary['Organism_name'].append(result['TSeq_orgname'])\n",
    "            dictionary['Description'].append(result['TSeq_defline'])\n",
    "            dictionary['Seq_length'].append(result['TSeq_length'])\n",
    "            dictionary['Prot_sequence'].append(result['TSeq_sequence'])\n",
    "        except KeyError:\n",
    "            logging.error('A sequence failed parsing from EFetch')\n",
    "            print('Could not parse one sequence from efetch')\n",
    "    logging.info(\"{} sequences were parsed correctly\".format(len(dictionary['Accession'])))\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_taxon(df:DataFrame, n_of_sequences = 1) -> DataFrame:\n",
    "    #Record initial length of DF\n",
    "    initial_len = len(df)\n",
    "\n",
    "    #Make a list of all retrieved taxons\n",
    "    retrieved_taxids = df['Taxid'].tolist() \n",
    "    #Make list from dictionary to eliminate duplicates \n",
    "    retrieved_taxids = list(dict.fromkeys(retrieved_taxids))\n",
    "        \n",
    "    #Declare empty DF\n",
    "    df_toreturn = pd.DataFrame()\n",
    "\n",
    "    #For each taxon create a DF, sort and get best result to append to df_toreturn\n",
    "    for taxid in retrieved_taxids:\n",
    "        #Create temporary DF exclusive to taxon\n",
    "        temp_df = df[df['Taxid'] == taxid]\n",
    "        temp_df = temp_df.sort_values(['Evalue', 'Identity', 'Bitscore'], ascending=[True, False, False]) #('Identity', ascending=False)\n",
    "        if len(temp_df) > 0:\n",
    "            if len(temp_df) > n_of_sequences:\n",
    "                df_toreturn = df_toreturn.append(temp_df[:n_of_sequences])\n",
    "            else:\n",
    "                df_toreturn = df_toreturn.append(temp_df)\n",
    "\n",
    "    #Refactor indexes\n",
    "    df_toreturn = df_toreturn.reset_index(drop=True)\n",
    "\n",
    "    #Calculate final length and append info to logging \n",
    "    final_len = len(df_toreturn)\n",
    "    logging.info(\"Filtering by taxon: {} unique taxid(s) collected, {} entries discarded\".format(len(retrieved_taxids), (initial_len - final_len)))\n",
    "    return df_toreturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare string for alingment file\n",
    "def fasta_for_alignment(query:SeqRecord, df:DataFrame) -> str:\n",
    "    #Initialise string and add \n",
    "    string = ''\n",
    "    if query.id != '<unknown id>':\n",
    "        string += \">Query:{}\\n\".format(query.id)\n",
    "    else:\n",
    "        string += \">Query\\n\"\n",
    "    string += \"{}\\n\".format(query.seq)\n",
    "    #Add all sequence IDs and aa sequence\n",
    "    for i in range(len(df['Accession'])):\n",
    "        #Check if protein is missing ------------------------------------------------Add to report? ERROR.LOG FILE!!!!!\n",
    "        if len(df['Prot_sequence'][i]) <1:\n",
    "            continue\n",
    "        string += \">{}__{}({})__{}\\n\".format(df['Protein_ID'][i], df['Organism_name'][i], df['Taxid'][i], df['Description'][i])\n",
    "        string += \"{}\".format(df['Prot_sequence'][i])\n",
    "        #Check if is not the last element in the list \n",
    "        if i != len(df['Accession']):\n",
    "            string += '\\n'\n",
    "    logging.info(\"{} sequences were prepared for alignment with the query\".format(len(df)))\n",
    "    return string\n",
    "\n",
    "\n",
    "\n",
    "#Change 'defining string' from protein to protein name/species name/taxid!!!!!!!!!!!!!!\n",
    "#For accession replace accession + name!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run mafft by saving the fasta sequences for alignment in a file and passing it to mafft \n",
    "def run_mafft_saving_file(fasta:str, mafft_directory:str, filename:str) -> str:\n",
    "    #Write fasta file for alignment\n",
    "    file = '{}.fasta'.format(filename)\n",
    "    with open(file, 'w') as handle:\n",
    "        handle.write(fasta)\n",
    "    #Parse and run with mafft \n",
    "    command_list = [mafft_directory, '--distout', '{}'.format(file)]\n",
    "    process = subprocess.Popen(command_list, universal_newlines= True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    #Save standard error of mafft\n",
    "    with open('aligned_mafft.stderr', 'w') as handle:\n",
    "        handle.write(stderr)\n",
    "    #Retrun alignment \n",
    "    return stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get protein sequence and save fasta file given an accession number\n",
    "def get_fasta_from_accession(accession, email):\n",
    "\n",
    "    #Get the efetch handler \n",
    "    Bio.Entrez.email = email\n",
    "    handler = Bio.Entrez.efetch(db='protein', id=accession, rettype = 'fasta',retmode = 'xml', retmax=1) #Returns JSON regardless\n",
    "    query_protein_efetch = Bio.Entrez.read(handler, 'text')#Returns nested lists and dictionaries \n",
    "    \n",
    "    #Make a dictionary -- passed as list to recycle the efetch_protein_to_dictionary function\n",
    "    dictionary_query = efetch_protein_to_dictionary([query_protein_efetch])\n",
    "\n",
    "    #Make and save fasta file \n",
    "    fasta_string_query = \">{} \\n{}\".format(dictionary_query['Accession'][0], dictionary_query['Prot_sequence'][0])\n",
    "\n",
    "    fasta_file_name = \"{}_sequence.fasta\".format(dictionary_query['Accession'][0])\n",
    "    with open(fasta_file_name, 'w') as handle:\n",
    "        handle.write(fasta_string_query)\n",
    "\n",
    "    #Open sequence with open_fasta and return it \n",
    "    fasta_record_q = open_fasta(fasta_file_name)\n",
    "    return fasta_record_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_input(input, email):\n",
    "    #Declare empty protein sequence\n",
    "    protein_sequence = None\n",
    "    #Check that the parsed string has a fasta extension, if it does pass it to open fasta function\n",
    "    if input.endswith('.fas') or input.endswith('.fasta'):\n",
    "        sequence = open_fasta(input)\n",
    "        #Try translating the sequence, if an error is raised the sequence is already a peptide and can be returned\n",
    "        try: \n",
    "            protein_sequence = sequence.translate(to_stop = True)\n",
    "            logging.info('Nucleotide sequence was translated to protein')\n",
    "        except Exception:\n",
    "            protein_sequence = sequence\n",
    "            logging.info('Protein sequence was opened')\n",
    "    #If the string doesn't have an extension it will be passed to the get_fasta_from_accession function\n",
    "    else:\n",
    "        try:\n",
    "            protein_sequence = get_fasta_from_accession(input, email)\n",
    "            logging.info('Protein sequence was retrieved from NCBI protein database')\n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            logging.error('The file must have a .fas or .fasta extension')\n",
    "\n",
    "    return protein_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_from_alignment(alignment):\n",
    "    calculator = DistanceCalculator('identity')\n",
    "    distance_matrix = calculator.get_distance(alignment)\n",
    "\n",
    "    constructor = DistanceTreeConstructor(calculator, 'nj')\n",
    "    tree = constructor.build_tree(alignment)\n",
    "    logging.info('Tree was produced with this and that method') #Placeholder to change\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USER INPUTS\n",
    "input_string = 'human_mx1.fas' #'sry_protein.fasta' 'QBA69874'\n",
    "taxid_list = [] #['9592']\n",
    "mafft_directory = r'/Users/Gioele/miniconda3/bin/mafft'\n",
    "email = 'A.N.Other@example.com'\n",
    "output_name = 'draft_v4_debug_3'\n",
    "\n",
    "\n",
    "local_query = False\n",
    "threading = False\n",
    "query_size = 100\n",
    "\n",
    "evalue_threshold = 10**-10\n",
    "len_threshold = 50\n",
    "identity_threshold = 50\n",
    "\n",
    "sequences_per_taxon = 1\n",
    "\n",
    "#Make tsv for figtree compatibility\n",
    "make_tsv = False\n",
    "\n",
    "#logging file \n",
    "logging.basicConfig(filename='{}.log'.format(output_name), filemode='w', format='%(levelname)s:%(message)s', level=logging.DEBUG) #logging refreshes every run and only displays type of message: message\n",
    "\n",
    "#Filenames:\n",
    "#Implemented\n",
    "output_df = '{}_df.csv'.format(output_name)\n",
    "output_alignment = '{}_alignment.fasta'.format(output_name)\n",
    "output_xml_tree = '{}_xml_tree.xml'.format(output_name)\n",
    "#To add\n",
    "output_tree_newick = '{}_newick_tree.nwk'.format(output_name) #!!!!\n",
    "output_tree_jpg = '{}_tree_image.jpg'.format(output_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Open fasta record\n",
    "fasta_record = open_input(input_string, email)\n",
    "#Blast with list \n",
    "blast_results = blastp_with_list(fasta_record.seq, query_size=query_size, list_taxid=taxid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Blast w threading \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAST DF after filtering holds 100 entries\n"
     ]
    }
   ],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEED TO DOUBLECHECK AND IMPLEMENT BLAST WITH THREADING \n",
    "\n",
    "\n",
    "#Parse handler to xml_string_to_handler to be able to feed it in SearchIO.read\n",
    "handler_blast = xml_string_to_handler(blast_results)\n",
    "\n",
    "#Make dictionary from handler\n",
    "dictionary_blast = blast_to_dictionary(handler_blast)\n",
    "\n",
    "#Transform dictionary into DF\n",
    "results_df_blast = pd.DataFrame.from_dict(dictionary_blast)\n",
    "\n",
    "\n",
    "#Filter DF by E-value / Bitscore / Identity\n",
    "filtered_blast_df = filter_df_blast(results_df_blast, fasta_record, evalue=evalue_threshold, difference_from_query=len_threshold, identity_threshold=identity_threshold)\n",
    "\n",
    "print(\"BLAST DF after filtering holds {} entries\".format(len(results_df_blast)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse one sequence from efetch\n",
      "Could not parse one sequence from efetch\n",
      "Filtered DF presents 23 entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Retrieve all results with efetch \n",
    "#Retrieve accession list\n",
    "accession_list = filtered_blast_df['Accession'].tolist()\n",
    "#Get results \n",
    "efetch_result = retrieve_all_efetch(accession_list, email)\n",
    "\n",
    "# Make dictionary from efetch fasta results\n",
    "dictionary_efetch = efetch_protein_to_dictionary(efetch_result)\n",
    "\n",
    "#Transform dictionary into DF\n",
    "efetch_df = pd.DataFrame.from_dict(dictionary_efetch)\n",
    "\n",
    "\n",
    "\n",
    "#Merge blast and efetch DataFrames\n",
    "left = filtered_blast_df.loc[:,['Accession', 'Seq_length', 'Evalue', 'Bitscore', 'Tot_aln_span', 'Identity']]\n",
    "right = efetch_df.loc[:,['Accession', 'Protein_ID' ,'Taxid', 'Organism_name', 'Description', 'Prot_sequence']]\n",
    "combined_df = pd.merge(left, right, on='Accession')\n",
    "\n",
    "\n",
    "#Filter DF based on taxons\n",
    "filtered_df = filter_df_taxon(combined_df, n_of_sequences=sequences_per_taxon)\n",
    "\n",
    "\n",
    "#Save DF of sequences that are going to be aligned \n",
    "filtered_df.to_csv(output_df, index = False)\n",
    "\n",
    "\n",
    "print('Filtered DF presents {} entries'.format(len(output_df)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make TSV for figtree\n",
    "tsv_df = filtered_df[['Protein_ID'] + [col for col in filtered_df.columns if col!= 'Protein_ID']]\n",
    "tsv_df = tsv_df.drop('Prot_sequence', axis = 1)\n",
    "tsv_df.to_csv('{}.tsv'.format(output_df), sep = '\\t', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare for multiple alingment\n",
    "fasta_string_for_alignment = fasta_for_alignment(fasta_record, filtered_df)\n",
    "\n",
    "#Run mafft alignment saving file\n",
    "mafft_alignment = run_mafft_saving_file(fasta_string_for_alignment, mafft_directory, 'multiple_seq_fasta')\n",
    "\n",
    "#Save mafft alignment\n",
    "with open(output_alignment, 'w') as savefile:\n",
    "    savefile.write(mafft_alignment)\n",
    "\n",
    "\n",
    "#Open AlignIO from fasta file \n",
    "alignment = AlignIO.read(output_alignment, 'fasta')\n",
    "\n",
    "#Construct tree from alignment \n",
    "#To IMPLEMENT DIFFERENT METHODS FOR TREE BUILDING \n",
    "tree = tree_from_alignment(alignment)  \n",
    "\n",
    "\n",
    "#Write tree in xml\n",
    "Phylo.write(tree, output_xml_tree, 'phyloxml')\n",
    "\n",
    "Phylo.write(tree, output_tree_newick, 'newick')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re import newick string to feed to ETE3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_tree_newick, 'r') as handle:\n",
    "    newick_string = handle.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newick_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import PhyloTree, TreeStyle\n",
    "\n",
    "#Link tree with MSA\n",
    "\n",
    "msa_tree = PhyloTree(newick_string, quoted_node_names=True, format=1) \n",
    "#Need to root the tree!!!!!!!\n",
    "#Returns the node that divides the current tree into two distance-balanced partitions.\n",
    "R = msa_tree.get_midpoint_outgroup()\n",
    "#Sets a descendant node as the outgroup of a tree\n",
    "msa_tree.set_outgroup(R)\n",
    "\n",
    "#Link tree to the MAFFT Alignment \n",
    "msa_tree.link_to_alignment(alignment=mafft_alignment, alg_format='fasta')\n",
    "msa_tree\n",
    "\n",
    "'''leaf_x = None\n",
    "for leaf in msa_tree.iter_leaves():\n",
    "    print('Name: {}, Sequence: {}'.format(leaf.name, leaf.sequence))\n",
    "    leaf_x=leaf'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_tree.show(tree_style=TreeStyle())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change 'defining string' from protein to protein name/species name/taxid!!!!!!!!!!!!!!\n",
    "#For accession replace accession + name!!!\n",
    "\n",
    "\n",
    "msa_tree.render('msa_tree_midpoint.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF you want to add the taxids you need to add the one of the query too\n",
    "#EXCEPTION!! SORRY THE FASTA FORMAT SHOULD PRESENT PROTEIN ID, FORMAT:\n",
    "#>gi|2765634|emb|Z78509.1|PPZ78509 P.pearcei 5.8S rRNA gene and ITS1 and ITS2 DNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "\n",
    "t = Tree( \"((a,b),c);\" )\n",
    "t.render(\"mytree.png\", w=183, units=\"mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newick_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "t = Tree(newick_string, quoted_node_names=True, format=1)\n",
    "t\n",
    "#t.render('my_newick_tree.png', w= 200, units='mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ete3 import NCBITaxa\n",
    "ncbi = NCBITaxa()\n",
    "descendants = ncbi.get_descendant_taxa(40674)\n",
    "# to print names of the taxIDs in the descendants\n",
    "print((descendants[:100]))\n",
    "\n",
    "\n",
    "#Very fast after first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check if the Taxid is a species one (so not to be parsed to the lineage)\n",
    "\n",
    "def check_if_species(list_of_taxa):\n",
    "    not_species = []\n",
    "    ranks = ncbi.get_rank(list_of_taxa)\n",
    "    for taxid in list_of_taxa:\n",
    "        if ranks[int(taxid)] != 'species':\n",
    "            not_species.append(taxid)\n",
    "    return not_species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxid_list = ['9592', '9606', '9597', '9593', '9600', '9601', '61853', '9546', '9544', '9541', '54180']\n",
    "\n",
    "not_species = check_if_species(taxid_list)\n",
    "print(not_species)\n",
    "\n",
    "\n",
    "species = []\n",
    "for taxid in not_species:\n",
    "    lineage_to_search = ncbi.get_lineage(int(taxid))\n",
    "    for spec in lineage_to_search:\n",
    "        species.append(int(spec))\n",
    "\n",
    "print(species)\n",
    "\n",
    "checked_species = check_if_species(species)\n",
    "\n",
    "\n",
    "ranks = ncbi.get_rank(species)\n",
    "\n",
    "\n",
    "print(ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = [163843, 109228, 98314, 98318, 109229, 163861, 196630, 1081385, 1704005, 65606, 65607, 65609, 65610, 65613, 65614, 65615, 65616, 65618, 65621, 65624, 65626, 65628, 65629, 65631, 65632, 65634, 1851651, 458857, 458858, 458859, 458860, 458861, 458862, 1048687, 1048688, 655476, 1649342, 2750743, 1851652, 2588810, 92866, 1392492, 65694, 1851653, 190511, 2162857, 2162858, 2162859, 2162860, 2162861, 2162862, 2162864, 2162865, 2162866, 2162901, 2162903, 2162904, 1147103, 1147105, 1147106, 655592, 491753, 557290, 491755, 1048812, 1048813, 1294088, 48018, 327948, 327957, 1966371, 1966372, 2588968, 2588969, 1605932, 1966381, 1966382, 1966383, 1966384, 1966385, 1513475, 131388, 262468, 491852, 491853, 1048917, 1048918, 1048919, 1048920, 1048921, 1048922, 1048923, 1081694, 491872, 491874, 491876, 491878, 1343853, 1343854, 491888]\n",
    "print(len(new_list))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea58ca30de0aa0442eb55bc2eea49e939cd8fed5b96958dd4f8b808b0201cbdd"
  },
  "kernelspec": {
   "display_name": "Python 3.5.6 64-bit ('ete3_2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
