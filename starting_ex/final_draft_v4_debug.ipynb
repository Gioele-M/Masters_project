{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "from sys import stderr, stdout\n",
    "import tempfile\n",
    "import subprocess\n",
    "from warnings import catch_warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO, SeqRecord, Seq, SearchIO, AlignIO, Phylo\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "import Bio.Entrez\n",
    "from Bio.Phylo.TreeConstruction import DistanceCalculator,DistanceTreeConstructor\n",
    "from pandas.core.frame import DataFrame\n",
    "import logging\n",
    "import time\n",
    "import traceback"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Function to open fasta file of imput\n",
    "def open_fasta(filename) -> SeqRecord:\n",
    "    with open(filename) as handle:\n",
    "        sequence_record = SeqIO.read(handle, 'fasta')\n",
    "    logging.info('Opened sequence {}'.format(sequence_record.id))\n",
    "    return sequence_record"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#Function to run BLAST with taxid list\n",
    "def blastp_with_list(sequence, list_taxid = [], query_size = 200):\n",
    "    result_handler, result_storer = None, None\n",
    "    #If list is empty run query without specific taxid\n",
    "    if len(list_taxid) <1:\n",
    "        result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, hitlist_size=query_size) \n",
    "        result_storer = result_handler.read()\n",
    "    #Prepare string of Entrez and parse it to qblast\n",
    "    else:\n",
    "        entrez_query = ''\n",
    "        for taxid in list_taxid:\n",
    "            entrez_query += 'txid{}[ORGN]'.format(taxid)\n",
    "            if taxid != list_taxid[-1]:\n",
    "                entrez_query += ' OR '\n",
    "        result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, entrez_query= entrez_query, hitlist_size=query_size)\n",
    "        result_storer = result_handler.read()\n",
    "    logging.info('BLASTp specifying {} taxid(s) completed'.format(len(list_taxid)))\n",
    "\n",
    "    return result_storer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "#Single blast query for threading\n",
    "#Threading to be implemented \n",
    "def blastp_single_taxid(sequence, taxid, query_size = 20):\n",
    "    entrez_query = 'txid{}[ORGN]'.format(taxid)\n",
    "    result_handler = NCBIWWW.qblast('blastp', 'nr', sequence, entrez_query= entrez_query, hitlist_size=query_size)    \n",
    "    result_storer = result_handler.read()\n",
    "    return result_storer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "#Function to create a temporary file from string and read it with SearchIO.read\n",
    "def xml_string_to_handler(string):\n",
    "    #make temporary file\n",
    "    tmp = tempfile.NamedTemporaryFile(mode='a+')\n",
    "    #write string\n",
    "    tmp.write(string)\n",
    "    handler = SearchIO.read(tmp.name, 'blast-xml')\n",
    "    tmp.close()\n",
    "    logging.info('Blast returned {} results'.format(len(handler)))\n",
    "    return handler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#Creation of a dictionary with all HSPS\n",
    "def blast_to_dictionary(blastresult):\n",
    "    blast_dictionary = {'ID' : [], 'Description' : [], 'Seq_length' : [], 'Accession' : [], 'Bitscore' : [], 'Evalue' : [], 'Tot_aln_span':[], 'Identity' :[]}\n",
    "    #Loop through results \n",
    "    for result in blastresult:\n",
    "        blast_dictionary['ID'].append(result.id)\n",
    "        blast_dictionary['Description'].append(result.description)\n",
    "        blast_dictionary['Seq_length'].append(result.seq_len)\n",
    "        blast_dictionary['Accession'].append(result.accession)\n",
    "        #Store results of first HSP\n",
    "        first_hsp = result.hsps[0]\n",
    "        blast_dictionary['Bitscore'].append(first_hsp.bitscore)\n",
    "        blast_dictionary['Evalue'].append(first_hsp.evalue)\n",
    "        #Create variables to store results of multiple hsps\n",
    "        all_alnspan, all_gapnum = [],[] \n",
    "        #Collect data of all hsps for each hit\n",
    "        for hsp in result.hsps:\n",
    "            all_alnspan.append(int(hsp.aln_span))\n",
    "            all_gapnum.append(int(hsp.gap_num))\n",
    "        #Calculate total alignment span and gaps to calculate identity\n",
    "        tot_alnspan, tot_gapnum = int(), int()\n",
    "        seq_len = int(result.seq_len) #DOUBLE CHECK \n",
    "        for span in all_alnspan:\n",
    "            tot_alnspan += span\n",
    "        for gap in all_gapnum:\n",
    "            tot_gapnum += gap\n",
    "        identity = ((tot_alnspan - tot_gapnum)/seq_len)*100\n",
    "        blast_dictionary['Tot_aln_span'].append(tot_alnspan)\n",
    "        blast_dictionary['Identity'].append(round(identity, 3))\n",
    "\n",
    "    logging.info(\"{} entries were recorded from the BLASTp results\".format(len(blast_dictionary['ID'])))\n",
    "    return blast_dictionary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Filter DF based on Evalue, sequence length and identity\n",
    "def filter_df_blast(df:DataFrame, query:SeqRecord, evalue = 10**-10, difference_from_query = 50, identity_threshold=50) -> DataFrame:\n",
    "    #Record initial dataframe length\n",
    "    initial_len = len(df)\n",
    "\n",
    "    remove_index = []\n",
    "    #Check if main HSP is significant for threshold, store indexes of non-significant hits\n",
    "    for i in range(len(df.index)):\n",
    "        eval = df.iloc[i]['Evalue']\n",
    "        if float(eval) > evalue:\n",
    "            remove_index.append(str(i))\n",
    "        \n",
    "    #Copy DF\n",
    "    df_to_return = df    \n",
    "    #Remove indexes if list is not empty\n",
    "    if len(remove_index)>0:\n",
    "        df_to_return = df_to_return.drop(df.index[int(remove_index)])\n",
    "        df_to_return = df_to_return.reset_index(drop=True)\n",
    "    \n",
    "    #Filter by +/- % of query length\n",
    "    #Obtain query length\n",
    "    query_length = len(query.seq)\n",
    "    #Determine upper/lower threshold of acceptance for sequences \n",
    "    lower, upper = query_length*(difference_from_query/100), query_length*((difference_from_query + 100)/100)\n",
    "    #Filter DF\n",
    "    df_to_return = df_to_return[df_to_return['Seq_length'] > lower]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "    df_to_return = df_to_return[df_to_return['Seq_length'] < upper]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "\n",
    "    #Filter by identity\n",
    "    df_to_return = df_to_return[df_to_return['Identity'] > identity_threshold]\n",
    "    df_to_return = df_to_return.reset_index(drop=True)\n",
    "    \n",
    "    #Calculate how many sequences were removed\n",
    "    final_len = len(df_to_return)\n",
    "\n",
    "    logging.info(\"{} sequences were removed filtering the BLAST results DF, returning a DF with {} entries\".format((initial_len-final_len),final_len))\n",
    "    return df_to_return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Retrieve all result's entries from efetch\n",
    "def retrieve_all_efetch(list_of_entries, email):\n",
    "    Bio.Entrez.email = email\n",
    "    list_of_sequences = []\n",
    "    for entry in list_of_entries:\n",
    "        handler = Bio.Entrez.efetch(db='protein', id=entry, rettype = 'fasta',retmode = 'xml', retmax=1) #Returns JSON regardless\n",
    "        gb_info = Bio.Entrez.read(handler, 'text')#Returns nested lists and dictionaries \n",
    "        list_of_sequences.append(gb_info)\n",
    "    logging.info('{} protein entries were retrieved from NCBI protein database'.format(len(list_of_sequences)))\n",
    "    return list_of_sequences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Efetch to dictionary parser\n",
    "def efetch_protein_to_dictionary(list_of_efetch):\n",
    "    #Declare new dictionary\n",
    "    dictionary = {'Accession':[],'Protein_ID':[], 'Taxid':[], 'Organism_name':[], 'Description':[], 'Seq_length':[], 'Prot_sequence':[]}\n",
    "    for wrapper in list_of_efetch:\n",
    "        try:\n",
    "            #Cast into dictionary to avoid random exception\n",
    "            result = dict(wrapper[0])\n",
    "            acc_ver = result['TSeq_accver']\n",
    "            accession = acc_ver.split('.')\n",
    "            dictionary['Accession'].append(accession[0])\n",
    "            dictionary['Protein_ID'].append(result['TSeq_accver'])\n",
    "            dictionary['Taxid'].append(result['TSeq_taxid'])\n",
    "            dictionary['Organism_name'].append(result['TSeq_orgname'])\n",
    "            dictionary['Description'].append(result['TSeq_defline'])\n",
    "            dictionary['Seq_length'].append(result['TSeq_length'])\n",
    "            dictionary['Prot_sequence'].append(result['TSeq_sequence'])\n",
    "        except KeyError:\n",
    "            logging.error('A sequence failed parsing from EFetch')\n",
    "            print('Could not parse one sequence from efetch')\n",
    "    logging.info(\"{} sequences were parsed correctly\".format(len(dictionary['Accession'])))\n",
    "    return dictionary"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def filter_df_taxon(df:DataFrame, n_of_sequences = 1) -> DataFrame:\n",
    "    #Record initial length of DF\n",
    "    initial_len = len(df)\n",
    "\n",
    "    #Make a list of all retrieved taxons\n",
    "    retrieved_taxids = df['Taxid'].tolist() \n",
    "    #Make list from dictionary to eliminate duplicates \n",
    "    retrieved_taxids = list(dict.fromkeys(retrieved_taxids))\n",
    "        \n",
    "    #Declare empty DF\n",
    "    df_toreturn = pd.DataFrame()\n",
    "\n",
    "    #For each taxon create a DF, sort and get best result to append to df_toreturn\n",
    "    for taxid in retrieved_taxids:\n",
    "        #Create temporary DF exclusive to taxon\n",
    "        temp_df = df[df['Taxid'] == taxid]\n",
    "        temp_df = temp_df.sort_values(['Evalue', 'Identity', 'Bitscore'], ascending=[True, False, False]) #('Identity', ascending=False)\n",
    "        if len(temp_df) > 0:\n",
    "            if len(temp_df) > n_of_sequences:\n",
    "                df_toreturn = df_toreturn.append(temp_df[:n_of_sequences])\n",
    "            else:\n",
    "                df_toreturn = df_toreturn.append(temp_df)\n",
    "\n",
    "    #Refactor indexes\n",
    "    df_toreturn = df_toreturn.reset_index(drop=True)\n",
    "\n",
    "    #Calculate final length and append info to logging \n",
    "    final_len = len(df_toreturn)\n",
    "    logging.info(\"Filtering by taxon: {} unique taxid(s) collected, {} entries discarded\".format(len(retrieved_taxids), (initial_len - final_len)))\n",
    "    return df_toreturn\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Prepare string for alingment file\n",
    "def fasta_for_alignment(query:SeqRecord, df:DataFrame) -> str:\n",
    "    #Initialise string and add \n",
    "    string = ''\n",
    "    string += \">Query:{}\\n\".format(query.id)\n",
    "    string += \"{}\\n\".format(query.seq)\n",
    "    #Add all sequence IDs and aa sequence\n",
    "    for i in range(len(df['Accession'])):\n",
    "        #Check if protein is missing ------------------------------------------------Add to report? ERROR.LOG FILE!!!!!\n",
    "        if len(df['Prot_sequence'][i]) <1:\n",
    "            continue\n",
    "        string += \">{}\\n\".format(df['ID'][i]) # {df['Organism_name'][i]} {df['Description'][i]}\n",
    "        string += \"{}\".format(df['Prot_sequence'][i])\n",
    "        #Check if is not the last element in the list \n",
    "        if i != len(df['Accession']):\n",
    "            string += '\\n'\n",
    "    logging.info(\"{} sequences were prepared for alignment with the query\".format(len(df)))\n",
    "    return string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Run mafft by saving the fasta sequences for alignment in a file and passing it to mafft \n",
    "def run_mafft_saving_file(fasta:str, mafft_directory:str, filename:str) -> str:\n",
    "    #Write fasta file for alignment\n",
    "    file = '{}.fasta'.format(filename)\n",
    "    with open(file, 'w') as handle:\n",
    "        handle.write(fasta)\n",
    "    #Parse and run with mafft \n",
    "    command_list = [mafft_directory, '--distout', '{}'.format(file)]\n",
    "    process = subprocess.Popen(command_list, universal_newlines= True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "\n",
    "    #Save standard error of mafft\n",
    "    with open('aligned_mafft.stderr', 'w') as handle:\n",
    "        handle.write(stderr)\n",
    "    #Retrun alignment \n",
    "    return stdout"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#Function to get protein sequence and save fasta file given an accession number\n",
    "def get_fasta_from_accession(accession, email):\n",
    "\n",
    "    #Get the efetch handler \n",
    "    Bio.Entrez.email = email\n",
    "    handler = Bio.Entrez.efetch(db='protein', id=accession, rettype = 'fasta',retmode = 'xml', retmax=1) #Returns JSON regardless\n",
    "    query_protein_efetch = Bio.Entrez.read(handler, 'text')#Returns nested lists and dictionaries \n",
    "    \n",
    "    #Make a dictionary -- passed as list to recycle the efetch_protein_to_dictionary function\n",
    "    dictionary_query = efetch_protein_to_dictionary([query_protein_efetch])\n",
    "\n",
    "    #Make and save fasta file \n",
    "    fasta_string_query = \">{} \\n{}\".format(dictionary_query['Accession'][0], dictionary_query['Prot_sequence'][0])\n",
    "\n",
    "    fasta_file_name = \"{}_sequence.fasta\".format(dictionary_query['Accession'][0])\n",
    "    with open(fasta_file_name, 'w') as handle:\n",
    "        handle.write(fasta_string_query)\n",
    "\n",
    "    #Open sequence with open_fasta and return it \n",
    "    fasta_record_q = open_fasta(fasta_file_name)\n",
    "    return fasta_record_q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def open_input(input, email):\n",
    "    #Declare empty protein sequence\n",
    "    protein_sequence = None\n",
    "    #Check that the parsed string has a fasta extension, if it does pass it to open fasta function\n",
    "    if input.endswith('.fas') or input.endswith('.fasta'):\n",
    "        sequence = open_fasta(input)\n",
    "        #Try translating the sequence, if an error is raised the sequence is already a peptide and can be returned\n",
    "        try: \n",
    "            protein_sequence = sequence.translate(to_stop = True)\n",
    "            logging.info('Nucleotide sequence was translated to protein')\n",
    "        except Exception:\n",
    "            protein_sequence = sequence\n",
    "            logging.info('Protein sequence was opened')\n",
    "    #If the string doesn't have an extension it will be passed to the get_fasta_from_accession function\n",
    "    else:\n",
    "        try:\n",
    "            protein_sequence = get_fasta_from_accession(input, email)\n",
    "            logging.info('Protein sequence was retrieved from NCBI protein database')\n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            logging.error('The file must have a .fas or .fasta extension')\n",
    "\n",
    "    return protein_sequence\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def tree_from_alignment(alignment):\n",
    "    calculator = DistanceCalculator('identity')\n",
    "    distance_matrix = calculator.get_distance(alignment)\n",
    "\n",
    "    constructor = DistanceTreeConstructor(calculator, 'nj')\n",
    "    tree = constructor.build_tree(alignment)\n",
    "    logging.info('Tree was produced with this and that method') #Placeholder to change\n",
    "    return tree"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "#USER INPUTS\n",
    "input_string = 'human_mx1.fas' #'sry_protein.fasta' 'QBA69874'\n",
    "taxid_list = [] #['9592']\n",
    "mafft_directory = r'/Users/Gioele/miniconda3/bin/mafft'\n",
    "email = 'A.N.Other@example.com'\n",
    "output_name = 'draft_v4_debug'\n",
    "\n",
    "\n",
    "local_query = False\n",
    "threading = False\n",
    "query_size = 100\n",
    "\n",
    "evalue_threshold = 10**-10\n",
    "len_threshold = 50\n",
    "identity_threshold = 50\n",
    "\n",
    "sequences_per_taxon = 1\n",
    "\n",
    "#Make tsv for figtree compatibility\n",
    "make_tsv = False\n",
    "\n",
    "#logging file \n",
    "logging.basicConfig(filename='{}.log'.format(output_name), filemode='w', format='%(levelname)s:%(message)s', level=logging.DEBUG) #logging refreshes every run and only displays type of message: message\n",
    "\n",
    "#Filenames:\n",
    "#Implemented\n",
    "output_df = '{}_df.csv'.format(output_name)\n",
    "output_alignment = '{}_alignment.fasta'.format(output_name)\n",
    "output_xml_tree = '{}_xml_tree.xml'.format(output_name)\n",
    "#To add\n",
    "output_tree_newick = '{}_newick_tree.nwk'.format(output_name) #!!!!\n",
    "output_tree_jpg = '{}_tree_image.jpg'.format(output_name)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\n",
    "#Open fasta record\n",
    "fasta_record = open_input(input_string, email)\n",
    "#Blast with list \n",
    "blast_results = blastp_with_list(fasta_record.seq, query_size=query_size, list_taxid=taxid_list)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "#Blast w threading \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!NEED TO DOUBLECHECK AND IMPLEMENT BLAST WITH THREADING \n",
    "\n",
    "\n",
    "#Parse handler to xml_string_to_handler to be able to feed it in SearchIO.read\n",
    "handler_blast = xml_string_to_handler(blast_results)\n",
    "\n",
    "#Make dictionary from handler\n",
    "dictionary_blast = blast_to_dictionary(handler_blast)\n",
    "\n",
    "#Transform dictionary into DF\n",
    "results_df_blast = pd.DataFrame.from_dict(dictionary_blast)\n",
    "\n",
    "\n",
    "#Filter DF by E-value / Bitscore / Identity\n",
    "filtered_blast_df = filter_df_blast(results_df_blast, fasta_record, evalue=evalue_threshold, difference_from_query=len_threshold, identity_threshold=identity_threshold)\n",
    "\n",
    "print(\"BLAST DF after filtering holds {} entries\".format(len(results_df_blast)))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BLAST DF after filtering holds 100 entries\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "#Retrieve all results with efetch \n",
    "#Retrieve accession list\n",
    "accession_list = filtered_blast_df['Accession'].tolist()\n",
    "#Get results \n",
    "efetch_result = retrieve_all_efetch(accession_list, email)\n",
    "\n",
    "# Make dictionary from efetch fasta results\n",
    "dictionary_efetch = efetch_protein_to_dictionary(efetch_result)\n",
    "\n",
    "#Transform dictionary into DF\n",
    "efetch_df = pd.DataFrame.from_dict(dictionary_efetch)\n",
    "\n",
    "\n",
    "\n",
    "#Merge blast and efetch DataFrames\n",
    "left = filtered_blast_df.loc[:,['Accession', 'ID','Seq_length', 'Evalue', 'Bitscore', 'Tot_aln_span', 'Identity']]\n",
    "right = efetch_df.loc[:,['Accession', 'Taxid', 'Organism_name', 'Description', 'Prot_sequence']]\n",
    "combined_df = pd.merge(left, right, on='Accession')\n",
    "\n",
    "\n",
    "#Filter DF based on taxons\n",
    "filtered_df = filter_df_taxon(combined_df, n_of_sequences=sequences_per_taxon)\n",
    "\n",
    "\n",
    "#Save DF of sequences that are going to be aligned \n",
    "filtered_df.to_csv(output_df, index = False)\n",
    "\n",
    "\n",
    "print('Filtered DF presents {} entries'.format(len(output_df)))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ERROR:root:A sequence failed parsing from EFetch\n",
      "ERROR:root:A sequence failed parsing from EFetch\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Could not parse one sequence from efetch\n",
      "Could not parse one sequence from efetch\n",
      "Filtered DF presents draft_v4_debug_df.csv entries\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "\n",
    "make_tsv = True\n",
    "\n",
    "#Check if TSV output is requested. Useful for FigTree\n",
    "if make_tsv:    \n",
    "    tsv_df = filtered_df[['ID'] + [col for col in filtered_df.columns if col!= 'ID']]\n",
    "    tsv_df.to_csv('{}.tsv'.format(output_df), sep = '\\t', index = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "#Prepare for multiple alingment\n",
    "fasta_string_for_alignment = fasta_for_alignment(fasta_record, filtered_df)\n",
    "\n",
    "#Run mafft alignment saving file\n",
    "mafft_alignment = run_mafft_saving_file(fasta_string_for_alignment, mafft_directory, 'multiple_seq_fasta')\n",
    "\n",
    "#Save mafft alignment\n",
    "with open(output_alignment, 'w') as savefile:\n",
    "    savefile.write(mafft_alignment)\n",
    "\n",
    "\n",
    "#Open AlignIO from fasta file \n",
    "alignment = AlignIO.read(output_alignment, 'fasta')\n",
    "\n",
    "#Construct tree from alignment \n",
    "#To IMPLEMENT DIFFERENT METHODS FOR TREE BUILDING \n",
    "tree = tree_from_alignment(alignment)  \n",
    "\n",
    "\n",
    "#Write tree in xml\n",
    "Phylo.write(tree, output_xml_tree, 'phyloxml')\n",
    "\n",
    "Phylo.write(tree, output_tree_newick, 'newick')\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "from ete3 import Tree\n",
    "\n",
    "\n",
    "t = Tree( \"((a,b),c);\" )\n",
    "t.render(\"mytree.png\", w=183, units=\"mm\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NewickError",
     "evalue": "Unexpected newick format 'Inner40:0.00433' \nYou may want to check other newick loading flags like 'format' or 'quoted_node_names'.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNewickError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6f0299f047c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewick_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mytree.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m183\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gioele/miniconda3/envs/ete3_2/lib/python3.5/site-packages/ete3-3.1.2-py3.7.egg/ete3/coretype/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, newick, format, dist, support, name, quoted_node_names)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             read_newick(newick, root_node = self, format=format,\n\u001b[0;32m--> 213\u001b[0;31m                         quoted_names=quoted_node_names)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gioele/miniconda3/envs/ete3_2/lib/python3.5/site-packages/ete3-3.1.2-py3.7.egg/ete3/parser/newick.py\u001b[0m in \u001b[0;36mread_newick\u001b[0;34m(newick, root_node, format, quoted_names)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNewickError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unexisting tree file or Malformed newick tree structure.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_read_newick_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_node\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoted_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gioele/miniconda3/envs/ete3_2/lib/python3.5/site-packages/ete3-3.1.2-py3.7.egg/ete3/parser/newick.py\u001b[0m in \u001b[0;36m_read_newick_from_string\u001b[0;34m(nw, root_node, matcher, formatcode, quoted_names)\u001b[0m\n\u001b[1;32m    339\u001b[0m                     \u001b[0mclosing_internal\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mclosing_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\";\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m                     \u001b[0;31m# read internal node data and go up one level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                     \u001b[0m_read_node_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_parent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"internal\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                     \u001b[0mcurrent_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Gioele/miniconda3/envs/ete3_2/lib/python3.5/site-packages/ete3-3.1.2-py3.7.egg/ete3/parser/newick.py\u001b[0m in \u001b[0;36m_read_node_data\u001b[0;34m(subnw, current_node, node_type, matcher, formatcode)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0m_parse_extra_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNewickError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected newick format '%s' \"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0msubnw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewickError\u001b[0m: Unexpected newick format 'Inner40:0.00433' \nYou may want to check other newick loading flags like 'format' or 'quoted_node_names'."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "with open(output_tree_newick, 'r') as handle:\n",
    "    newick_string = handle.read()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.5.6",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "pygments_lexer": "ipython3"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.5.6 64-bit ('ete3_2': conda)"
  },
  "interpreter": {
   "hash": "42fe78aebb72ae4ffa9c0879ec3daa3a9473ae9ee57e2d93cd6d9a5311e615e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}